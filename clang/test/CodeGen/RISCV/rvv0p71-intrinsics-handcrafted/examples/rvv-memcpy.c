// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py UTC_ARGS: --version 2
// RUN: %clang_cc1 -triple riscv64 -target-feature +xtheadvector \
// RUN: -O0 -emit-llvm %s -o - | FileCheck %s

#include <riscv_vector.h>

typedef unsigned char uint8_t;

// CHECK-LABEL: define dso_local void @memcpy_v
// CHECK-SAME: (ptr noundef [[DST:%.*]], ptr noundef [[SRC:%.*]], i32 noundef signext [[N:%.*]]) #[[ATTR0:[0-9]+]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DST_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[SRC_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[N_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[VL:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[VEC_SRC:%.*]] = alloca <vscale x 32 x i8>, align 1
// CHECK-NEXT:    store ptr [[DST]], ptr [[DST_ADDR]], align 8
// CHECK-NEXT:    store ptr [[SRC]], ptr [[SRC_ADDR]], align 8
// CHECK-NEXT:    store i32 [[N]], ptr [[N_ADDR]], align 4
// CHECK-NEXT:    br label [[FOR_COND:%.*]]
// CHECK:       for.cond:
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[N_ADDR]], align 4
// CHECK-NEXT:    [[CMP:%.*]] = icmp sgt i32 [[TMP0]], 0
// CHECK-NEXT:    br i1 [[CMP]], label [[FOR_BODY:%.*]], label [[FOR_END:%.*]]
// CHECK:       for.body:
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[N_ADDR]], align 4
// CHECK-NEXT:    [[CONV:%.*]] = sext i32 [[TMP1]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = call i64 @llvm.riscv.th.vsetvl.i64(i64 [[CONV]], i64 0, i64 2)
// CHECK-NEXT:    [[CONV1:%.*]] = trunc i64 [[TMP2]] to i32
// CHECK-NEXT:    store i32 [[CONV1]], ptr [[VL]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[SRC_ADDR]], align 8
// CHECK-NEXT:    [[TMP4:%.*]] = load i32, ptr [[VL]], align 4
// CHECK-NEXT:    [[CONV2:%.*]] = sext i32 [[TMP4]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = call <vscale x 32 x i8> @llvm.riscv.th.vle.nxv32i8.i64(<vscale x 32 x i8> poison, ptr [[TMP3]], i64 [[CONV2]])
// CHECK-NEXT:    store <vscale x 32 x i8> [[TMP5]], ptr [[VEC_SRC]], align 1
// CHECK-NEXT:    [[TMP6:%.*]] = load ptr, ptr [[DST_ADDR]], align 8
// CHECK-NEXT:    [[TMP7:%.*]] = load <vscale x 32 x i8>, ptr [[VEC_SRC]], align 1
// CHECK-NEXT:    [[TMP8:%.*]] = load i32, ptr [[VL]], align 4
// CHECK-NEXT:    [[CONV3:%.*]] = sext i32 [[TMP8]] to i64
// CHECK-NEXT:    call void @llvm.riscv.th.vse.nxv32i8.i64(<vscale x 32 x i8> [[TMP7]], ptr [[TMP6]], i64 [[CONV3]])
// CHECK-NEXT:    br label [[FOR_INC:%.*]]
// CHECK:       for.inc:
// CHECK-NEXT:    [[TMP9:%.*]] = load i32, ptr [[VL]], align 4
// CHECK-NEXT:    [[TMP10:%.*]] = load i32, ptr [[N_ADDR]], align 4
// CHECK-NEXT:    [[SUB:%.*]] = sub nsw i32 [[TMP10]], [[TMP9]]
// CHECK-NEXT:    store i32 [[SUB]], ptr [[N_ADDR]], align 4
// CHECK-NEXT:    [[TMP11:%.*]] = load i32, ptr [[VL]], align 4
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[SRC_ADDR]], align 8
// CHECK-NEXT:    [[IDX_EXT:%.*]] = sext i32 [[TMP11]] to i64
// CHECK-NEXT:    [[ADD_PTR:%.*]] = getelementptr inbounds i8, ptr [[TMP12]], i64 [[IDX_EXT]]
// CHECK-NEXT:    store ptr [[ADD_PTR]], ptr [[SRC_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = load i32, ptr [[VL]], align 4
// CHECK-NEXT:    [[TMP14:%.*]] = load ptr, ptr [[DST_ADDR]], align 8
// CHECK-NEXT:    [[IDX_EXT4:%.*]] = sext i32 [[TMP13]] to i64
// CHECK-NEXT:    [[ADD_PTR5:%.*]] = getelementptr inbounds i8, ptr [[TMP14]], i64 [[IDX_EXT4]]
// CHECK-NEXT:    store ptr [[ADD_PTR5]], ptr [[DST_ADDR]], align 8
// CHECK-NEXT:    br label [[FOR_COND]], !llvm.loop [[LOOP4:![0-9]+]]
// CHECK:       for.end:
// CHECK-NEXT:    ret void
//
void memcpy_v(uint8_t *dst, const uint8_t *src, int n) {
  for (int vl; n > 0; n -= vl, src += vl, dst += vl) {
    vl = __riscv_vsetvl_e8m4(n);
    vuint8m4_t vec_src = __riscv_th_vle8_v_u8m4(src, vl);
    __riscv_th_vse8_v_u8m4(dst, vec_src, vl);
  }
}
