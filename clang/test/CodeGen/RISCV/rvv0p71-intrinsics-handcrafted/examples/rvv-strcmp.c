// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py UTC_ARGS: --version 2
// RUN: %clang_cc1 -triple riscv64 -target-feature +xtheadvector \
// RUN:   -disable-O0-optnone -emit-llvm %s -o - | \
// RUN:   opt -S -passes=mem2reg | \
// RUN:   FileCheck --check-prefix=CHECK-IR %s

#include <riscv_vector.h>

// CHECK-IR-LABEL: define dso_local signext i32 @strcmp_vec
// CHECK-IR-SAME: (ptr noundef [[SOURCE1:%.*]], ptr noundef [[SOURCE2:%.*]]) #[[ATTR0:[0-9]+]] {
// CHECK-IR-NEXT:  entry:
// CHECK-IR-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.th.vsetvlmax.i64(i64 0, i64 1)
// CHECK-IR-NEXT:    br label [[FOR_COND:%.*]]
// CHECK-IR:       for.cond:
// CHECK-IR-NEXT:    [[SRC2_0:%.*]] = phi ptr [ [[SOURCE2]], [[ENTRY:%.*]] ], [ [[ADD_PTR1:%.*]], [[FOR_INC:%.*]] ]
// CHECK-IR-NEXT:    [[SRC1_0:%.*]] = phi ptr [ [[SOURCE1]], [[ENTRY]] ], [ [[ADD_PTR:%.*]], [[FOR_INC]] ]
// CHECK-IR-NEXT:    [[FIRST_SET_BIT_0:%.*]] = phi i64 [ -1, [[ENTRY]] ], [ [[TMP10:%.*]], [[FOR_INC]] ]
// CHECK-IR-NEXT:    [[VL_0:%.*]] = phi i64 [ undef, [[ENTRY]] ], [ [[TMP6:%.*]], [[FOR_INC]] ]
// CHECK-IR-NEXT:    [[CMP:%.*]] = icmp slt i64 [[FIRST_SET_BIT_0]], 0
// CHECK-IR-NEXT:    br i1 [[CMP]], label [[FOR_BODY:%.*]], label [[FOR_END:%.*]]
// CHECK-IR:       for.body:
// CHECK-IR-NEXT:    [[TMP1:%.*]] = call { <vscale x 16 x i8>, i64 } @llvm.riscv.th.vleff.nxv16i8.i64(<vscale x 16 x i8> poison, ptr [[SRC1_0]], i64 [[TMP0]])
// CHECK-IR-NEXT:    [[TMP2:%.*]] = extractvalue { <vscale x 16 x i8>, i64 } [[TMP1]], 0
// CHECK-IR-NEXT:    [[TMP3:%.*]] = extractvalue { <vscale x 16 x i8>, i64 } [[TMP1]], 1
// CHECK-IR-NEXT:    [[TMP4:%.*]] = call { <vscale x 16 x i8>, i64 } @llvm.riscv.th.vleff.nxv16i8.i64(<vscale x 16 x i8> poison, ptr [[SRC2_0]], i64 [[TMP3]])
// CHECK-IR-NEXT:    [[TMP5:%.*]] = extractvalue { <vscale x 16 x i8>, i64 } [[TMP4]], 0
// CHECK-IR-NEXT:    [[TMP6]] = extractvalue { <vscale x 16 x i8>, i64 } [[TMP4]], 1
// CHECK-IR-NEXT:    [[TMP7:%.*]] = call <vscale x 16 x i1> @llvm.riscv.th.vmseq.nxv16i8.i8.i64(<vscale x 16 x i8> [[TMP2]], i8 0, i64 [[TMP6]])
// CHECK-IR-NEXT:    [[TMP8:%.*]] = call <vscale x 16 x i1> @llvm.riscv.th.vmsne.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> [[TMP2]], <vscale x 16 x i8> [[TMP5]], i64 [[TMP6]])
// CHECK-IR-NEXT:    [[TMP9:%.*]] = call <vscale x 16 x i1> @llvm.riscv.th.vmor.nxv16i1.i64(<vscale x 16 x i1> [[TMP7]], <vscale x 16 x i1> [[TMP8]], i64 [[TMP6]])
// CHECK-IR-NEXT:    [[TMP10]] = call i64 @llvm.riscv.th.vmfirst.nxv16i1.i64(<vscale x 16 x i1> [[TMP9]], i64 [[TMP6]])
// CHECK-IR-NEXT:    br label [[FOR_INC]]
// CHECK-IR:       for.inc:
// CHECK-IR-NEXT:    [[ADD_PTR]] = getelementptr inbounds i8, ptr [[SRC1_0]], i64 [[TMP6]]
// CHECK-IR-NEXT:    [[ADD_PTR1]] = getelementptr inbounds i8, ptr [[SRC2_0]], i64 [[TMP6]]
// CHECK-IR-NEXT:    br label [[FOR_COND]], !llvm.loop [[LOOP4:![0-9]+]]
// CHECK-IR:       for.end:
// CHECK-IR-NEXT:    [[SUB:%.*]] = sub i64 [[VL_0]], [[FIRST_SET_BIT_0]]
// CHECK-IR-NEXT:    [[IDX_NEG:%.*]] = sub i64 0, [[SUB]]
// CHECK-IR-NEXT:    [[ADD_PTR2:%.*]] = getelementptr inbounds i8, ptr [[SRC1_0]], i64 [[IDX_NEG]]
// CHECK-IR-NEXT:    [[SUB3:%.*]] = sub i64 [[VL_0]], [[FIRST_SET_BIT_0]]
// CHECK-IR-NEXT:    [[IDX_NEG4:%.*]] = sub i64 0, [[SUB3]]
// CHECK-IR-NEXT:    [[ADD_PTR5:%.*]] = getelementptr inbounds i8, ptr [[SRC2_0]], i64 [[IDX_NEG4]]
// CHECK-IR-NEXT:    [[TMP11:%.*]] = load i8, ptr [[ADD_PTR2]], align 1
// CHECK-IR-NEXT:    [[CONV:%.*]] = zext i8 [[TMP11]] to i32
// CHECK-IR-NEXT:    [[TMP12:%.*]] = load i8, ptr [[ADD_PTR5]], align 1
// CHECK-IR-NEXT:    [[CONV6:%.*]] = zext i8 [[TMP12]] to i32
// CHECK-IR-NEXT:    [[SUB7:%.*]] = sub nsw i32 [[CONV]], [[CONV6]]
// CHECK-IR-NEXT:    ret i32 [[SUB7]]
//
int strcmp_vec(const char *source1, const char *source2) {
  const unsigned char *src1 = (const void*)source1, *src2 = (const void*)source2;
  size_t vlmax = __riscv_vsetvlmax_e8m2();
  long first_set_bit = -1;
  size_t vl;
  for (; first_set_bit < 0; src1 += vl, src2 += vl) {
    vuint8m2_t vec_src1 = __riscv_vle8ff_v_u8m2(src1, &vl, vlmax);
    vuint8m2_t vec_src2 = __riscv_vle8ff_v_u8m2(src2, &vl, vl);

    vbool4_t string_terminate = __riscv_vmseq_vx_u8m2_b4(vec_src1, 0, vl);
    vbool4_t no_equal = __riscv_vmsne_vv_u8m2_b4(vec_src1, vec_src2, vl);
    vbool4_t vec_terminate = __riscv_vmor_mm_b4(string_terminate, no_equal, vl);

    first_set_bit = __riscv_vfirst_m_b4(vec_terminate, vl);
  }
  src1 -= vl - first_set_bit;
  src2 -= vl - first_set_bit;
  return *src1 - *src2;
}
